# AIがコードを書く時代、Gitだけでは監査できない

## Claude Code + Entire で「なぜこのコードが生まれたか」を追跡する

## はじめに

Claude Code や Cursor を使って開発していると、こんな経験はありませんか？

- PRの差分が大きすぎてレビューできない
- なぜその実装になったのか説明できない
- バグの原因が人かAIか分からない
- 同じコードを再生成できない

これはスキルの問題ではありません。

**AIコーディングによって、ソフトウェア開発の前提が変わりつつあるためです。**

---

## Gitは「人間が書く前提」で設計されている

Gitが記録するのは：

- 誰が変更したか
- 何を変更したか（diff）

しかしAI開発で本当に重要なのは

> なぜこのコードが生成されたのか

です。

AIによる生成の流れはこうなります。

プロンプト → 推論 → 外部ツール → 生成コード → commit

Gitが保存するのは最後の commit だけです。

つまり、 **最も重要な「生成過程」が記録されていません。**

---

## 再現性の消失

従来のバグ調査：

コード → 実行 → 再現

AI開発のバグ調査：

プロンプト → モデル状態 → 外部コンテキスト → ツール実行 → 生成

同じコードを再生成できないケースが発生します。

これは偶然ではなく、LLMの性質です。

つまり、

> 出力されたコードだけでは原因を特定できない

フォレンジック（事後調査）が難しくなります。

---

## レビューが成立しなくなる

従来のレビューは

- 設計意図の確認
- バグ検出

を目的としていました。

しかしAI生成コードでは

- 差分が巨大
- 一見整っている
- もっともらしい

結果としてレビューは

「問題なさそう」

という確認に変わりがちです。

これは検証ではありません。

レビュー対象は **コードそのものではなく、判断過程** に移っています。

---

## 必要なのは「意思決定履歴」

従来：変更履歴 これから：判断履歴

必要になる情報：

- プロンプト
- AIの応答
- 参照した情報
- 実行されたツール

つまり コードではなく「生成過程」を追跡できる必要があります。

---

## 解決アプローチの一例

この問題を解決するアプローチはいくつかあります。

- 生成ログの保存
- エージェント監査
- セッション記録

その一例として Entire があります。

Entire は、AIセッション（プロンプト・応答・ツール実行）をコミットと紐づけて保存する仕組みです。

管理対象 ツール

---

成果物 Git 判断過程 Entireなどのセッション記録

---

## 最小導入

    entire enable

Git hooks が追加され、 AIセッションがコミットに紐付けられます。

あとは普段通り：

- ブランチ作成
- Claude Codeで実装
- commit
- PR

---

## 導入時の補助設定

PRにAIセッションを表示する方法： → PRテンプレート記事へ

CIからログ用ブランチを除外する方法： → GitHub Actions設定記事へ

---

## まとめ

AIコーディング時代に必要なのは

変更履歴ではなく

**意思決定履歴です。**

Gitだけでは「何が変わったか」は分かります。しかし「なぜ変わったか」は分かりません。

AI開発では、この差が品質と責任分界を左右します。
